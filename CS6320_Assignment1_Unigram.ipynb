{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS6320 - NLP - Assignment 1 - Group 14\n",
        "1. Sai Vikas Thiruveedula (SXT230026)\n",
        "2. Vijay Sai Dukkipati (DXV220040)\n",
        "3. Manikanta Sai Kommireddy (MXK220132)"
      ],
      "metadata": {
        "id": "YsolUpZobkQ_"
      },
      "id": "YsolUpZobkQ_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the Dataset"
      ],
      "metadata": {
        "id": "UqAcoPXQgX1l"
      },
      "id": "UqAcoPXQgX1l"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url=\"https://raw.githubusercontent.com/saivikas10/Assignment/refs/heads/main/A1_DATASET/train.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "\n",
        "with open('train.txt', 'w') as file:\n",
        "    file.write(response.text)\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/saivikas10/Assignment/refs/heads/main/A1_DATASET/val.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "\n",
        "with open('val.txt', 'w') as file:\n",
        "    file.write(response.text)"
      ],
      "metadata": {
        "id": "-8MU7I1ddBQr"
      },
      "id": "-8MU7I1ddBQr",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries and Define Helper Functions"
      ],
      "metadata": {
        "id": "FNBErlVBcU-5"
      },
      "id": "FNBErlVBcU-5"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Global dictionary for storing the results and to display at last\n",
        "results = {}\n",
        "\n",
        "#Perplexity calculation function\n",
        "def calculate_perplexity(train_prob, val_token, train_log_prob):\n",
        "    val_uni_count = defaultdict(int)\n",
        "    val_word_count = 0\n",
        "    for word in val_token:\n",
        "        val_uni_count[word] += 1\n",
        "        val_word_count += 1\n",
        "\n",
        "    sum_log_prob = 0\n",
        "    for word in val_uni_count:\n",
        "        #condition for handling unknown words\n",
        "        if word not in train_log_prob:\n",
        "            word_log_prob = train_log_prob['unk']\n",
        "        else:\n",
        "            word_log_prob = train_log_prob[word]\n",
        "        sum_log_prob += (-1) * word_log_prob * val_uni_count[word]\n",
        "\n",
        "    perplexity = math.exp(sum_log_prob / val_word_count)\n",
        "    return perplexity\n",
        "\n",
        "#add_k_smoothing function for computing probabilities\n",
        "def add_k_smoothing(train_token, k):\n",
        "    k_uni_count, k_uni_word_count = defaultdict(int), 0\n",
        "    for word in train_token:\n",
        "        k_uni_count[word] += 1\n",
        "        k_uni_word_count += 1\n",
        "\n",
        "    k_uni_train_prob = defaultdict(int)\n",
        "    for word in k_uni_count:\n",
        "        k_uni_train_prob[word] = (k_uni_count[word] + k) / (k_uni_word_count + k * len(k_uni_count))\n",
        "\n",
        "    return k_uni_train_prob, k_uni_count, k_uni_word_count\n",
        "\n",
        "print(\"Libraries imported and helper functions defined.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClFMHhYoa7d0",
        "outputId": "9d3bb559-3630-4d54-c666-71be3211ab63"
      },
      "id": "ClFMHhYoa7d0",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and helper functions defined.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading and Preprocessing Data"
      ],
      "metadata": {
        "id": "mep-ntU5cZGt"
      },
      "id": "mep-ntU5cZGt"
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_preprocess_data(train_file, val_file):\n",
        "    print(\"-------------------------\")\n",
        "    print(\"Step 1: Reading and displaying the first 200 characters of training and validation data\")\n",
        "    print(\"-------------------------\")\n",
        "\n",
        "    #loading training data\n",
        "    with open(train_file, 'r') as file:\n",
        "        train = file.read()\n",
        "    print(\"First 200 characters of Training Data:\\n\", train[:200], \"\\n\")\n",
        "\n",
        "    train = re.sub(r'\\W', ' ', train)\n",
        "    train_token = train.lower().split()\n",
        "\n",
        "    #loading validation data\n",
        "    with open(val_file, 'r') as file:\n",
        "        val = file.read()\n",
        "    print(\"First 200 characters of Validation Data:\\n\", val[:200], \"\\n\")\n",
        "\n",
        "    val = re.sub(r'\\W', ' ', val)\n",
        "    val_token = val.lower().split()\n",
        "\n",
        "    #sample data display\n",
        "    print(\"Training Data Tokens (Top 10):\")\n",
        "    for token in train_token[:10]:\n",
        "        print(token)\n",
        "    print(\"\\nValidation Data Tokens (Top 10):\")\n",
        "    for token in val_token[:10]:\n",
        "        print(token)\n",
        "\n",
        "    #calculating total number of tokens in training and validation data\n",
        "    print(f\"Total number of tokens in Training Data: {len(train_token)}\")\n",
        "    print(f\"Total number of tokens in Validation Data: {len(val_token)}\")\n",
        "\n",
        "    #calculating the total number of unique tokens\n",
        "    print(f\"Number of unique tokens in Training Data: {len(set(train_token))}\")\n",
        "\n",
        "    return train_token, val_token\n",
        "\n",
        "#Preprocessing\n",
        "train_token, val_token = read_and_preprocess_data('train.txt', 'val.txt')\n",
        "print(\"Training and validation data have been preprocessed and tokens extracted.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HjoskbkbHnM",
        "outputId": "65240c31-4ca0-4ccc-991d-615af33daf19"
      },
      "id": "7HjoskbkbHnM",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------\n",
            "Step 1: Reading and displaying the first 200 characters of training and validation data\n",
            "-------------------------\n",
            "First 200 characters of Training Data:\n",
            " I booked two rooms four months in advance at the Talbott . We were placed on the top floor next to the elevators , which are used all night long . When speaking to the front desk , I was told that the \n",
            "\n",
            "First 200 characters of Validation Data:\n",
            " I stayed for four nights while attending a conference . The hotel is in a great spot - easy walk to Michigan Ave shopping or Rush St. , but just off the busy streets . The room I had was spacious , an \n",
            "\n",
            "Training Data Tokens (Top 10):\n",
            "i\n",
            "booked\n",
            "two\n",
            "rooms\n",
            "four\n",
            "months\n",
            "in\n",
            "advance\n",
            "at\n",
            "the\n",
            "\n",
            "Validation Data Tokens (Top 10):\n",
            "i\n",
            "stayed\n",
            "for\n",
            "four\n",
            "nights\n",
            "while\n",
            "attending\n",
            "a\n",
            "conference\n",
            "the\n",
            "Total number of tokens in Training Data: 80300\n",
            "Total number of tokens in Validation Data: 8835\n",
            "Number of unique tokens in Training Data: 5962\n",
            "Training and validation data have been preprocessed and tokens extracted.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculating Unsmoothed Unigram Probabilities and Log Probabilities"
      ],
      "metadata": {
        "id": "i815Fct9cgaZ"
      },
      "id": "i815Fct9cgaZ"
    },
    {
      "cell_type": "code",
      "source": [
        "def unsmoothed_unigram_probabilities(train_token):\n",
        "    print(\"\\n--- Calculating Unsmoothed Unigram Probabilities ---\")\n",
        "\n",
        "    #counting the word frequencies and calculating probabilities\n",
        "    un_uni_count, un_uni_word_count = defaultdict(int), 0\n",
        "    for word in train_token:\n",
        "        un_uni_count[word] += 1\n",
        "        un_uni_word_count += 1\n",
        "    print(f\"Total Words (Training Data): {un_uni_word_count}\")\n",
        "    print(f\"Unique Words (Training Data): {len(un_uni_count)}\")\n",
        "\n",
        "    print(f\"Word Counts (Training Data, Top 15):\")\n",
        "    for word, count in list(un_uni_count.items())[:15]:\n",
        "        print(f\"{word}: {count}\")\n",
        "\n",
        "    un_uni_train_prob = defaultdict(int)\n",
        "    for word in un_uni_count:\n",
        "        un_uni_train_prob[word] = un_uni_count[word] / un_uni_word_count\n",
        "\n",
        "    print(f\"\\nUnsmoothed Unigram Probabilities (Top 15):\")\n",
        "    for word, prob in list(un_uni_train_prob.items())[:15]:\n",
        "        print(f\"{word}: {prob:.6f}\")\n",
        "\n",
        "    freq_distribution = Counter(train_token)\n",
        "    top_10_prob = {word: un_uni_train_prob[word] for word, _ in freq_distribution.most_common(10)}\n",
        "    print(f\"\\nProbabilities of Top 10 Most Frequent Words:\")\n",
        "    for word, prob in top_10_prob.items():\n",
        "        print(f\"{word}: {prob:.6f}\")\n",
        "\n",
        "    # calculating Log of probabilities\n",
        "    un_uni_train_log_prob = defaultdict(int)\n",
        "    for word in un_uni_train_prob:\n",
        "        un_uni_train_log_prob[word] = math.log(un_uni_train_prob[word])\n",
        "\n",
        "    print(f\"\\nLog Probabilities (Top 15):\")\n",
        "    for word, log_prob in list(un_uni_train_log_prob.items())[:15]:\n",
        "        print(f\"{word}: {log_prob:.6f}\")\n",
        "\n",
        "    return un_uni_train_prob, un_uni_train_log_prob\n",
        "\n",
        "# Running Unsmoothed Unigram Model and Perplexity Calculation\n",
        "un_uni_train_prob, un_uni_train_log_prob = unsmoothed_unigram_probabilities(train_token)\n",
        "print(\"Unsmoothed Unigram Probabilities and Log Probabilities calculated.\\n\")\n",
        "\n",
        "# Running Perplexity of Training Dataset Without Any Smoothing\n",
        "perplexity_unsmoothed = calculate_perplexity(un_uni_train_prob, train_token, un_uni_train_log_prob)\n",
        "results['Perplexity (Training Dataset Without Any Smoothing)'] = perplexity_unsmoothed\n",
        "print(f\"Perplexity (Training Dataset Without Any Smoothing): {perplexity_unsmoothed}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl58-S1ubNLn",
        "outputId": "63deb01b-6700-41ca-d2c8-8ba3fbc40301"
      },
      "id": "rl58-S1ubNLn",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating Unsmoothed Unigram Probabilities ---\n",
            "Total Words (Training Data): 80300\n",
            "Unique Words (Training Data): 5962\n",
            "Word Counts (Training Data, Top 15):\n",
            "i: 1722\n",
            "booked: 86\n",
            "two: 129\n",
            "rooms: 203\n",
            "four: 21\n",
            "months: 8\n",
            "in: 1318\n",
            "advance: 7\n",
            "at: 745\n",
            "the: 5315\n",
            "talbott: 28\n",
            "we: 1117\n",
            "were: 578\n",
            "placed: 8\n",
            "on: 644\n",
            "\n",
            "Unsmoothed Unigram Probabilities (Top 15):\n",
            "i: 0.021445\n",
            "booked: 0.001071\n",
            "two: 0.001606\n",
            "rooms: 0.002528\n",
            "four: 0.000262\n",
            "months: 0.000100\n",
            "in: 0.016413\n",
            "advance: 0.000087\n",
            "at: 0.009278\n",
            "the: 0.066189\n",
            "talbott: 0.000349\n",
            "we: 0.013910\n",
            "were: 0.007198\n",
            "placed: 0.000100\n",
            "on: 0.008020\n",
            "\n",
            "Probabilities of Top 10 Most Frequent Words:\n",
            "the: 0.066189\n",
            "and: 0.032316\n",
            "a: 0.028057\n",
            "to: 0.026027\n",
            "was: 0.022740\n",
            "i: 0.021445\n",
            "in: 0.016413\n",
            "we: 0.013910\n",
            "of: 0.013051\n",
            "hotel: 0.012914\n",
            "\n",
            "Log Probabilities (Top 15):\n",
            "i: -3.842283\n",
            "booked: -6.839178\n",
            "two: -6.433712\n",
            "rooms: -5.980319\n",
            "four: -8.249002\n",
            "months: -9.214083\n",
            "in: -4.109654\n",
            "advance: -9.347615\n",
            "at: -4.680141\n",
            "the: -2.715237\n",
            "talbott: -7.961320\n",
            "we: -4.275123\n",
            "were: -4.933951\n",
            "placed: -9.214083\n",
            "on: -4.825826\n",
            "Unsmoothed Unigram Probabilities and Log Probabilities calculated.\n",
            "\n",
            "Perplexity (Training Dataset Without Any Smoothing): 519.796634745634\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Log Probability Distribution for Different Models"
      ],
      "metadata": {
        "id": "RHsGAvJmclCN"
      },
      "id": "RHsGAvJmclCN"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLog Probability Distribution (Unsmoothed Model - Top 10):\")\n",
        "for word, log_prob in list(un_uni_train_log_prob.items())[:10]:\n",
        "    print(f\"{word}: {log_prob:.6f}\")\n",
        "\n",
        "#calculating Laplace smoothed log probabilities for the training data\n",
        "lap_uni_count = defaultdict(int)\n",
        "lap_uni_word_count = 0\n",
        "for word in train_token:\n",
        "    lap_uni_count[word] += 1\n",
        "    lap_uni_word_count += 1\n",
        "\n",
        "lap_uni_train_prob = defaultdict(int)\n",
        "for word in lap_uni_count:\n",
        "    lap_uni_train_prob[word] = (lap_uni_count[word] + 1) / (lap_uni_word_count + len(lap_uni_count))\n",
        "\n",
        "lap_uni_train_log_prob = defaultdict(int)\n",
        "for word in lap_uni_train_prob:\n",
        "    lap_uni_train_log_prob[word] = math.log(lap_uni_train_prob[word])\n",
        "\n",
        "print(\"\\nLog Probability Distribution (Laplace Smoothing - Top 10):\")\n",
        "for word, log_prob in list(lap_uni_train_log_prob.items())[:10]:\n",
        "    print(f\"{word}: {log_prob:.6f}\")\n",
        "\n",
        "# Calculating Add-k where k=0.5 smoothed log probabilities for the training set\n",
        "k = 0.5\n",
        "k_uni_train_prob, k_uni_count, k_uni_word_count = add_k_smoothing(train_token, k)\n",
        "k_uni_train_log_prob = defaultdict(int)\n",
        "for word in k_uni_train_prob:\n",
        "    k_uni_train_log_prob[word] = math.log(k_uni_train_prob[word])\n",
        "\n",
        "print(\"\\nLog Probability Distribution (Add-k Smoothing, k=0.5 - Top 10):\")\n",
        "for word, log_prob in list(k_uni_train_log_prob.items())[:10]:\n",
        "    print(f\"{word}: {log_prob:.6f}\")\n",
        "\n",
        "# Calculating Add-k where k=3 smoothed log probabilities for the training set\n",
        "k = 3\n",
        "k_uni_train_prob, k_uni_count, k_uni_word_count = add_k_smoothing(train_token, k)\n",
        "k_uni_train_log_prob = defaultdict(int)\n",
        "for word in k_uni_train_prob:\n",
        "    k_uni_train_log_prob[word] = math.log(k_uni_train_prob[word])\n",
        "\n",
        "print(\"\\nLog Probability Distribution (Add-k Smoothing, k=3 - Top 10):\")\n",
        "for word, log_prob in list(k_uni_train_log_prob.items())[:10]:\n",
        "    print(f\"{word}: {log_prob:.6f}\")\n",
        "\n",
        "print(\"Log probability distributions for all models displayed.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2nODb4ibT5l",
        "outputId": "d9a98ed7-c227-47e5-a988-4607a6cb09bc"
      },
      "id": "Z2nODb4ibT5l",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Log Probability Distribution (Unsmoothed Model - Top 10):\n",
            "i: -3.842283\n",
            "booked: -6.839178\n",
            "two: -6.433712\n",
            "rooms: -5.980319\n",
            "four: -8.249002\n",
            "months: -9.214083\n",
            "in: -4.109654\n",
            "advance: -9.347615\n",
            "at: -4.680141\n",
            "the: -2.715237\n",
            "\n",
            "Log Probability Distribution (Laplace Smoothing - Top 10):\n",
            "i: -3.913322\n",
            "booked: -6.899236\n",
            "two: -6.497610\n",
            "rooms: -6.047024\n",
            "four: -8.274102\n",
            "months: -9.167920\n",
            "in: -4.180515\n",
            "advance: -9.285703\n",
            "at: -4.750419\n",
            "the: -2.786668\n",
            "\n",
            "Log Probability Distribution (Add-k Smoothing, k=0.5 - Top 10):\n",
            "i: -3.878444\n",
            "booked: -6.869831\n",
            "two: -6.466295\n",
            "rooms: -6.014310\n",
            "four: -8.261923\n",
            "months: -9.189910\n",
            "in: -4.145726\n",
            "advance: -9.315073\n",
            "at: -4.715921\n",
            "the: -2.751593\n",
            "\n",
            "Log Probability Distribution (Add-k Smoothing, k=3 - Top 10):\n",
            "i: -4.041637\n",
            "booked: -7.005983\n",
            "two: -6.611817\n",
            "rooms: -6.166743\n",
            "four: -8.316565\n",
            "months: -9.096724\n",
            "in: -4.308475\n",
            "advance: -9.192034\n",
            "at: -4.877216\n",
            "the: -2.915766\n",
            "Log probability distributions for all models displayed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Add-k Smoothing and Perplexity Calculation"
      ],
      "metadata": {
        "id": "3dccNatUcmWm"
      },
      "id": "3dccNatUcmWm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity of Training Dataset Using Laplace Smoothing Without Unknown Word Handling\n",
        "perplexity_laplace_without_unknown = calculate_perplexity(lap_uni_train_prob, train_token, lap_uni_train_log_prob)\n",
        "results['Perplexity (Laplace Smoothing Without Unknown)'] = perplexity_laplace_without_unknown\n",
        "print(f\"Perplexity (Laplace Smoothing Without Unknown): {perplexity_laplace_without_unknown}\")\n",
        "\n",
        "# Unigram Add-k Smoothing for k=0.5 without Unknown Word Handling\n",
        "k = 0.5\n",
        "k_uni_train_prob, k_uni_count, k_uni_word_count = add_k_smoothing(train_token, k)\n",
        "k_uni_train_log_prob = defaultdict(int)\n",
        "for word in k_uni_train_prob:\n",
        "    k_uni_train_log_prob[word] = math.log(k_uni_train_prob[word])\n",
        "perplexity_add_k_0_5 = calculate_perplexity(k_uni_train_prob, val_token, k_uni_train_log_prob)\n",
        "results['Perplexity (Add-k Smoothing k=0.5 without Unknown)'] = perplexity_add_k_0_5\n",
        "print(f\"Perplexity (Add-k Smoothing k=0.5 without Unknown): {perplexity_add_k_0_5}\")\n",
        "\n",
        "# Unigram Add-k Smoothing for k=3 without Unknown Word Handling\n",
        "k = 3\n",
        "k_uni_train_prob, k_uni_count, k_uni_word_count = add_k_smoothing(train_token, k)\n",
        "k_uni_train_log_prob = defaultdict(int)\n",
        "for word in k_uni_train_prob:\n",
        "    k_uni_train_log_prob[word] = math.log(k_uni_train_prob[word])\n",
        "perplexity_add_k_3 = calculate_perplexity(k_uni_train_prob, val_token, k_uni_train_log_prob)\n",
        "results['Perplexity (Add-k Smoothing k=3 without Unknown)'] = perplexity_add_k_3\n",
        "print(f\"Perplexity (Add-k Smoothing k=3 without Unknown): {perplexity_add_k_3}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCpql6xHbXJ6",
        "outputId": "3f43369b-8e38-4b7f-8a03-e0217d1df80b"
      },
      "id": "pCpql6xHbXJ6",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity (Laplace Smoothing Without Unknown): 526.2184224551577\n",
            "Perplexity (Add-k Smoothing k=0.5 without Unknown): 383.25196453239204\n",
            "Perplexity (Add-k Smoothing k=3 without Unknown): 413.50606773548816\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Laplace and Add-k Smoothing with Unknown Word Handling"
      ],
      "metadata": {
        "id": "hwYYv2V0c1rj"
      },
      "id": "hwYYv2V0c1rj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplace Smoothing with Unknown Word Handling\n",
        "lap_uni_count['unk'] = 0\n",
        "lap_uni_train_prob['unk'] = (lap_uni_count['unk'] + 1) / (lap_uni_word_count + len(lap_uni_count))\n",
        "lap_uni_train_log_prob['unk'] = math.log(lap_uni_train_prob['unk'])\n",
        "perplexity_laplace_unknown_handling = calculate_perplexity(lap_uni_train_prob, val_token, lap_uni_train_log_prob)\n",
        "results['Perplexity (Laplace Smoothing with Unknown)'] = perplexity_laplace_unknown_handling\n",
        "print(f\"Perplexity (Laplace Smoothing with Unknown): {perplexity_laplace_unknown_handling}\")\n",
        "\n",
        "# Add-k Smoothing for k=0.5 with Unknown Word Handling\n",
        "k = 0.5\n",
        "k_uni_count['unk'] = 0\n",
        "k_uni_train_prob['unk'] = (k_uni_count['unk'] + k) / (k_uni_word_count + (k * len(k_uni_count)))\n",
        "k_uni_train_log_prob['unk'] = math.log(k_uni_train_prob['unk'])\n",
        "perplexity_add_k_0_5_unknown = calculate_perplexity(k_uni_train_prob, val_token, k_uni_train_log_prob)\n",
        "results['Perplexity (Add-k Smoothing k=0.5 with Unknown)'] = perplexity_add_k_0_5_unknown\n",
        "print(f\"Perplexity (Add-k Smoothing k=0.5 with Unknown): {perplexity_add_k_0_5_unknown}\")\n",
        "\n",
        "# Add-k Smoothing for k=3 with Unknown Word Handling\n",
        "k = 3\n",
        "k_uni_count['unk'] = 0\n",
        "k_uni_train_prob['unk'] = (k_uni_count['unk'] + k) / (k_uni_word_count + (k * len(k_uni_count)))\n",
        "k_uni_train_log_prob['unk'] = math.log(k_uni_train_prob['unk'])\n",
        "perplexity_add_k_3_unknown = calculate_perplexity(k_uni_train_prob, val_token, k_uni_train_log_prob)\n",
        "results['Perplexity (Add-k Smoothing k=3 with Unknown)'] = perplexity_add_k_3_unknown\n",
        "print(f\"Perplexity (Add-k Smoothing k=3 with Unknown): {perplexity_add_k_3_unknown}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic6nqzjhbbO1",
        "outputId": "8982bbe7-e317-4ee1-e8d4-bf55c54dee4e"
      },
      "id": "Ic6nqzjhbbO1",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity (Laplace Smoothing with Unknown): 539.9572567347413\n",
            "Perplexity (Add-k Smoothing k=0.5 with Unknown): 585.8421256872612\n",
            "Perplexity (Add-k Smoothing k=3 with Unknown): 558.862953194595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Smoothing with Rare Words Tagged as Unknown"
      ],
      "metadata": {
        "id": "TQP0WVbac402"
      },
      "id": "TQP0WVbac402"
    },
    {
      "cell_type": "code",
      "source": [
        "# Unigram Laplace Smoothing with Rare Words as Unknown\n",
        "new_uni_count = defaultdict(int)\n",
        "new_uni_word_count = 0\n",
        "new_uni_count['unk'] = 0\n",
        "for word in train_token:\n",
        "    #Tagging rare words as unk\n",
        "    if lap_uni_count[word] == 1:\n",
        "        new_uni_count['unk'] += 1\n",
        "    else:\n",
        "        new_uni_count[word] += 1\n",
        "    new_uni_word_count += 1\n",
        "\n",
        "lap_uni_train_prob = defaultdict(int)\n",
        "for word in new_uni_count:\n",
        "    lap_uni_train_prob[word] = (new_uni_count[word] + 1) / (new_uni_word_count + len(new_uni_count))\n",
        "\n",
        "lap_uni_train_log_prob = defaultdict(int)\n",
        "for word in lap_uni_train_prob:\n",
        "    lap_uni_train_log_prob[word] = math.log(lap_uni_train_prob[word])\n",
        "lap_uni_train_log_prob['unk'] = math.log(lap_uni_train_prob['unk'])\n",
        "\n",
        "perplexity_laplace_rare_words = calculate_perplexity(lap_uni_train_prob, val_token, lap_uni_train_log_prob)\n",
        "results['Perplexity (Laplace Smoothing with Rare Words as Unknown)'] = perplexity_laplace_rare_words\n",
        "print(f\"Perplexity (Laplace Smoothing with Rare Words as Unknown): {perplexity_laplace_rare_words}\")\n",
        "\n",
        "# Add-k Smoothing for k=0.5 with Rare Words as Unknown\n",
        "k = 0.5\n",
        "k_uni_train_prob = defaultdict(int)\n",
        "for word in new_uni_count:\n",
        "    k_uni_train_prob[word] = (new_uni_count[word] + k) / (new_uni_word_count + (k * len(new_uni_count)))\n",
        "\n",
        "k_uni_train_log_prob = defaultdict(int)\n",
        "for word in k_uni_train_prob:\n",
        "    k_uni_train_log_prob[word] = math.log(k_uni_train_prob[word])\n",
        "\n",
        "perplexity_add_k_0_5_rare = calculate_perplexity(k_uni_train_prob, val_token, k_uni_train_log_prob)\n",
        "results['Perplexity (Add-k Smoothing k=0.5 with Rare Words as Unknown)'] = perplexity_add_k_0_5_rare\n",
        "print(f\"Perplexity (Add-k Smoothing k=0.5 with Rare Words as Unknown): {perplexity_add_k_0_5_rare}\")\n",
        "\n",
        "# Add-k Smoothing for k=3 with Rare Words as Unknown\n",
        "k = 3\n",
        "k_uni_train_prob = defaultdict(int)\n",
        "for word in new_uni_count:\n",
        "    k_uni_train_prob[word] = (new_uni_count[word] + k) / (new_uni_word_count + (k * len(new_uni_count)))\n",
        "\n",
        "k_uni_train_log_prob = defaultdict(int)\n",
        "for word in k_uni_train_prob:\n",
        "    k_uni_train_log_prob[word] = math.log(k_uni_train_prob[word])\n",
        "\n",
        "perplexity_add_k_3_rare = calculate_perplexity(k_uni_train_prob, val_token, k_uni_train_log_prob)\n",
        "results['Perplexity (Add-k Smoothing k=3 with Rare Words as Unknown)'] = perplexity_add_k_3_rare\n",
        "print(f\"Perplexity (Add-k Smoothing k=3 with Rare Words as Unknown): {perplexity_add_k_3_rare}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTLkIOwfbfCW",
        "outputId": "f4736eb6-3979-46a2-f131-ba278a34b1f0"
      },
      "id": "fTLkIOwfbfCW",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity (Laplace Smoothing with Rare Words as Unknown): 349.0340273743812\n",
            "Perplexity (Add-k Smoothing k=0.5 with Rare Words as Unknown): 347.33315472616965\n",
            "Perplexity (Add-k Smoothing k=3 with Rare Words as Unknown): 357.80478479349983\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final Step - Displaying All Results"
      ],
      "metadata": {
        "id": "ZhmDj1C2c99R"
      },
      "id": "ZhmDj1C2c99R"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final Step: Displaying results for all cases (Laplace, Add-k=0.5, Add-k=3)\")\n",
        "print(\"-------------------------\")\n",
        "for key, value in results.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\nAll results have been displayed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP3NBwy18Cfs",
        "outputId": "a0c5484d-ba30-4889-d848-abf9a644914b"
      },
      "id": "cP3NBwy18Cfs",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Step: Displaying results for all cases (Laplace, Add-k=0.5, Add-k=3)\n",
            "-------------------------\n",
            "Perplexity (Training Dataset Without Any Smoothing): 519.796634745634\n",
            "Perplexity (Laplace Smoothing Without Unknown): 526.2184224551577\n",
            "Perplexity (Add-k Smoothing k=0.5 without Unknown): 383.25196453239204\n",
            "Perplexity (Add-k Smoothing k=3 without Unknown): 413.50606773548816\n",
            "Perplexity (Laplace Smoothing with Unknown): 539.9572567347413\n",
            "Perplexity (Add-k Smoothing k=0.5 with Unknown): 585.8421256872612\n",
            "Perplexity (Add-k Smoothing k=3 with Unknown): 558.862953194595\n",
            "Perplexity (Laplace Smoothing with Rare Words as Unknown): 349.0340273743812\n",
            "Perplexity (Add-k Smoothing k=0.5 with Rare Words as Unknown): 347.33315472616965\n",
            "Perplexity (Add-k Smoothing k=3 with Rare Words as Unknown): 357.80478479349983\n",
            "\n",
            "All results have been displayed.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}