{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS6320 - NLP - Assignment 1 - Group 14\n",
        "1. Sai Vikas Thiruveedula (SXT230026)\n",
        "2. Vijay Sai Dukkipati (DXV220040)\n",
        "3. Manikanta Sai Kommireddy (MXK220132)"
      ],
      "metadata": {
        "id": "UpvOu7itwTVc"
      },
      "id": "UpvOu7itwTVc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing the Dataset"
      ],
      "metadata": {
        "id": "_WQgNCGWwWWT"
      },
      "id": "_WQgNCGWwWWT"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url=\"https://raw.githubusercontent.com/saivikas10/Assignment/refs/heads/main/A1_DATASET/train.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "\n",
        "with open('train.txt', 'w') as file:\n",
        "    file.write(response.text)\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/saivikas10/Assignment/refs/heads/main/A1_DATASET/val.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "\n",
        "with open('val.txt', 'w') as file:\n",
        "    file.write(response.text)"
      ],
      "metadata": {
        "id": "re4iqoDXwQSE"
      },
      "id": "re4iqoDXwQSE",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries and Define Helper Functions"
      ],
      "metadata": {
        "id": "3b_ccNesxPEe"
      },
      "id": "3b_ccNesxPEe"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "# Global dictionary for storing the results and to display at last\n",
        "results = {}\n",
        "\n",
        "#Perplexity calculation function\n",
        "def calculate_perplexity_bigram(train_prob, val_bigram, train_log_prob):\n",
        "    val_bi_count = defaultdict(int)\n",
        "    val_word_count = 0\n",
        "    for word in val_bigram:\n",
        "        val_bi_count[word] += 1\n",
        "        val_word_count += 1\n",
        "\n",
        "    sum_log_prob = 0\n",
        "    for word in val_bi_count:\n",
        "        ##condition for handling unknown bigrams\n",
        "        if word not in train_log_prob:\n",
        "            word_log_prob = train_log_prob.get(('unk', 'unk'), 0)\n",
        "        else:\n",
        "            word_log_prob = train_log_prob[word]\n",
        "        sum_log_prob += (-1) * word_log_prob * val_bi_count[word]\n",
        "\n",
        "    perplexity = math.exp(sum_log_prob / val_word_count)\n",
        "    return perplexity\n",
        "\n",
        "#Preprocessing\n",
        "def preprocess_and_get_bigrams(text):\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    tokens = text.lower().split()\n",
        "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)]\n",
        "    return tokens, bigrams\n",
        "\n",
        "print(\"Libraries imported and helper functions defined.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oDkQZT7weKS",
        "outputId": "7145ede9-783a-430f-a1f0-403edcfc303d"
      },
      "id": "1oDkQZT7weKS",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and helper functions defined.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading and Preprocessing Training and Validation Data"
      ],
      "metadata": {
        "id": "RpF7XdTSxVRm"
      },
      "id": "RpF7XdTSxVRm"
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_preprocess_data(train_file, val_file):\n",
        "    #loading training data\n",
        "    with open(train_file, 'r') as file:\n",
        "        train_text = file.read()\n",
        "    print(\"First 200 characters of Training Data:\\n\", train_text[:200], \"\\n\")\n",
        "\n",
        "    train_tokens, train_bigrams = preprocess_and_get_bigrams(train_text)\n",
        "\n",
        "    #loading validation data\n",
        "    with open(val_file, 'r') as file:\n",
        "        val_text = file.read()\n",
        "    print(\"First 200 characters of Validation Data:\\n\", val_text[:200], \"\\n\")\n",
        "\n",
        "    val_tokens, val_bigrams = preprocess_and_get_bigrams(val_text)\n",
        "\n",
        "    #sample data display\n",
        "    print(\"Training Data Tokens (Top 10):\")\n",
        "    for token in train_tokens[:10]:\n",
        "        print(token)\n",
        "    print(\"\\nValidation Data Tokens (Top 10):\")\n",
        "    for token in val_tokens[:10]:\n",
        "        print(token)\n",
        "\n",
        "    print(f\"Total number of tokens in Training Data: {len(train_tokens)}\")\n",
        "    print(f\"Total number of tokens in Validation Data: {len(val_tokens)}\")\n",
        "\n",
        "    print(f\"Number of unique tokens in Training Data: {len(set(train_tokens))}\")\n",
        "\n",
        "    return train_tokens, val_tokens, train_bigrams, val_bigrams\n",
        "\n",
        "train_tokens, val_tokens, train_bigrams, val_bigrams = read_and_preprocess_data('train.txt', 'val.txt')\n",
        "print(\"Training and validation data have been preprocessed and tokens extracted.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEbQP1qbwkgC",
        "outputId": "8da6845b-49f7-4767-fe83-6848855b8251"
      },
      "id": "vEbQP1qbwkgC",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 200 characters of Training Data:\n",
            " I booked two rooms four months in advance at the Talbott . We were placed on the top floor next to the elevators , which are used all night long . When speaking to the front desk , I was told that the \n",
            "\n",
            "First 200 characters of Validation Data:\n",
            " I stayed for four nights while attending a conference . The hotel is in a great spot - easy walk to Michigan Ave shopping or Rush St. , but just off the busy streets . The room I had was spacious , an \n",
            "\n",
            "Training Data Tokens (Top 10):\n",
            "i\n",
            "booked\n",
            "two\n",
            "rooms\n",
            "four\n",
            "months\n",
            "in\n",
            "advance\n",
            "at\n",
            "the\n",
            "\n",
            "Validation Data Tokens (Top 10):\n",
            "i\n",
            "stayed\n",
            "for\n",
            "four\n",
            "nights\n",
            "while\n",
            "attending\n",
            "a\n",
            "conference\n",
            "the\n",
            "Total number of tokens in Training Data: 80300\n",
            "Total number of tokens in Validation Data: 8835\n",
            "Number of unique tokens in Training Data: 5962\n",
            "Training and validation data have been preprocessed and tokens extracted.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculating Unsmoothed Bigram Probabilities and Log Probabilities"
      ],
      "metadata": {
        "id": "enPlsD9nxZro"
      },
      "id": "enPlsD9nxZro"
    },
    {
      "cell_type": "code",
      "source": [
        "def unsmoothed_bigram_probabilities(train_tokens, train_bigrams):\n",
        "    print(\"\\n--- Calculating Unsmoothed Bigram Probabilities ---\")\n",
        "\n",
        "    unigram_count, bigram_count = defaultdict(int), defaultdict(int)\n",
        "    for token in train_tokens:\n",
        "        unigram_count[token] += 1\n",
        "\n",
        "    for bigram in train_bigrams:\n",
        "        bigram_count[bigram] += 1\n",
        "\n",
        "    bigram_prob = defaultdict(int)\n",
        "    for bigram in bigram_count:\n",
        "        bigram_prob[bigram] = bigram_count[bigram] / unigram_count[bigram[0]]\n",
        "\n",
        "    print(f\"Bigram Counts (Top 15):\")\n",
        "    for bigram, count in list(bigram_count.items())[:15]:\n",
        "        print(f\"{bigram}: {count}\")\n",
        "\n",
        "    print(f\"\\nUnsmoothed Bigram Probabilities (Top 15):\")\n",
        "    for bigram, prob in list(bigram_prob.items())[:15]:\n",
        "        print(f\"{bigram}: {prob:.6f}\")\n",
        "\n",
        "    bigram_log_prob = defaultdict(int)\n",
        "    for bigram in bigram_prob:\n",
        "        bigram_log_prob[bigram] = math.log(bigram_prob[bigram])\n",
        "\n",
        "    print(f\"\\nLog Probabilities (Top 15):\")\n",
        "    for bigram, log_prob in list(bigram_log_prob.items())[:15]:\n",
        "        print(f\"{bigram}: {log_prob:.6f}\")\n",
        "\n",
        "    return bigram_prob, bigram_log_prob, unigram_count, bigram_count\n",
        "\n",
        "#counting unigram count and bigram count in the main code block\n",
        "bigram_prob, bigram_log_prob, unigram_count, bigram_count = unsmoothed_bigram_probabilities(train_tokens, train_bigrams)\n",
        "print(\"Unsmoothed Bigram Probabilities and Log Probabilities calculated.\\n\")\n",
        "\n",
        "\n",
        "#Calculating perplexity of training data without any smoothing\n",
        "perplexity_unsmoothed = calculate_perplexity_bigram(bigram_prob, train_bigrams, bigram_log_prob)\n",
        "results['Perplexity (Training Dataset Without Any Smoothing)'] = perplexity_unsmoothed\n",
        "print(f\"Perplexity (Training Dataset Without Any Smoothing): {perplexity_unsmoothed}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZVIVmxVwo9t",
        "outputId": "7ce6011d-8099-459d-c942-896a7f9bf1f7"
      },
      "id": "CZVIVmxVwo9t",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Calculating Unsmoothed Bigram Probabilities ---\n",
            "Bigram Counts (Top 15):\n",
            "('i', 'booked'): 21\n",
            "('booked', 'two'): 1\n",
            "('two', 'rooms'): 3\n",
            "('rooms', 'four'): 1\n",
            "('four', 'months'): 1\n",
            "('months', 'in'): 2\n",
            "('in', 'advance'): 7\n",
            "('advance', 'at'): 1\n",
            "('at', 'the'): 335\n",
            "('the', 'talbott'): 26\n",
            "('talbott', 'we'): 2\n",
            "('we', 'were'): 179\n",
            "('were', 'placed'): 1\n",
            "('placed', 'on'): 2\n",
            "('on', 'the'): 234\n",
            "\n",
            "Unsmoothed Bigram Probabilities (Top 15):\n",
            "('i', 'booked'): 0.012195\n",
            "('booked', 'two'): 0.011628\n",
            "('two', 'rooms'): 0.023256\n",
            "('rooms', 'four'): 0.004926\n",
            "('four', 'months'): 0.047619\n",
            "('months', 'in'): 0.250000\n",
            "('in', 'advance'): 0.005311\n",
            "('advance', 'at'): 0.142857\n",
            "('at', 'the'): 0.449664\n",
            "('the', 'talbott'): 0.004892\n",
            "('talbott', 'we'): 0.071429\n",
            "('we', 'were'): 0.160251\n",
            "('were', 'placed'): 0.001730\n",
            "('placed', 'on'): 0.250000\n",
            "('on', 'the'): 0.363354\n",
            "\n",
            "Log Probabilities (Top 15):\n",
            "('i', 'booked'): -4.406719\n",
            "('booked', 'two'): -4.454347\n",
            "('two', 'rooms'): -3.761200\n",
            "('rooms', 'four'): -5.313206\n",
            "('four', 'months'): -3.044522\n",
            "('months', 'in'): -1.386294\n",
            "('in', 'advance'): -5.237961\n",
            "('advance', 'at'): -1.945910\n",
            "('at', 'the'): -0.799254\n",
            "('the', 'talbott'): -5.320192\n",
            "('talbott', 'we'): -2.639057\n",
            "('we', 'were'): -1.831016\n",
            "('were', 'placed'): -6.359574\n",
            "('placed', 'on'): -1.386294\n",
            "('on', 'the'): -1.012378\n",
            "Unsmoothed Bigram Probabilities and Log Probabilities calculated.\n",
            "\n",
            "Perplexity (Training Dataset Without Any Smoothing): 30.21189738227987\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perplexity Calculations for Various Cases of Laplace Smoothing, Add-k smoothing with K=0.5 and K=3"
      ],
      "metadata": {
        "id": "mPgrnWPRxep3"
      },
      "id": "mPgrnWPRxep3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Smoothing and Perplexity Calculations Ignoring Unknown Words"
      ],
      "metadata": {
        "id": "WUKwdK98A_rb"
      },
      "id": "WUKwdK98A_rb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Laplace Smoothing\n",
        "print(\"\\n--- Adding Ignoring Unknown Words ---\")\n",
        "laplace_bigram_prob, laplace_bigram_log_prob = defaultdict(int), defaultdict(int)\n",
        "for bigram in bigram_count:\n",
        "    laplace_bigram_prob[bigram] = (bigram_count[bigram] + 1) / (unigram_count[bigram[0]] + len(unigram_count))\n",
        "    laplace_bigram_log_prob[bigram] = math.log(laplace_bigram_prob[bigram])\n",
        "\n",
        "perplexity_laplace = calculate_perplexity_bigram(laplace_bigram_prob, val_bigrams, laplace_bigram_log_prob)\n",
        "results['Perplexity (Laplace Smoothing)'] = perplexity_laplace\n",
        "print(f\"Perplexity (Laplace Smoothing - Ignoring Unknown Words): {perplexity_laplace}\")\n",
        "\n",
        "# Add-k Smoothing where k = 0.5\n",
        "k = 0.5\n",
        "k_bigram_prob, k_bigram_log_prob = defaultdict(int), defaultdict(int)\n",
        "for bigram in bigram_count:\n",
        "    k_bigram_prob[bigram] = (bigram_count[bigram] + k) / (unigram_count[bigram[0]] + (k * len(unigram_count)))\n",
        "    k_bigram_log_prob[bigram] = math.log(k_bigram_prob[bigram])\n",
        "\n",
        "perplexity_add_k_0_5 = calculate_perplexity_bigram(k_bigram_prob, val_bigrams, k_bigram_log_prob)\n",
        "results['Perplexity (Add-k Smoothing k=0.5)'] = perplexity_add_k_0_5\n",
        "print(f\"Perplexity (Add-k Smoothing, k=0.5 - Ignoring Unknown Words): {perplexity_add_k_0_5}\")\n",
        "\n",
        "# Add-k Smoothing where k = 3\n",
        "k = 3\n",
        "k_bigram_prob, k_bigram_log_prob = defaultdict(int), defaultdict(int)\n",
        "for bigram in bigram_count:\n",
        "    k_bigram_prob[bigram] = (bigram_count[bigram] + k) / (unigram_count[bigram[0]] + (k * len(unigram_count)))\n",
        "    k_bigram_log_prob[bigram] = math.log(k_bigram_prob[bigram])\n",
        "\n",
        "perplexity_add_k_3 = calculate_perplexity_bigram(k_bigram_prob, val_bigrams, k_bigram_log_prob)\n",
        "results['Perplexity (Add-k Smoothing k=3)'] = perplexity_add_k_3\n",
        "print(f\"Perplexity (Add-k Smoothing, k=3 - Ignoring Unknown Words): {perplexity_add_k_3}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgiU1R8sAvpk",
        "outputId": "79f9bb47-b036-4743-bdad-5e89db0bba73"
      },
      "id": "lgiU1R8sAvpk",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Adding Ignoring Unknown Words ---\n",
            "Perplexity (Laplace Smoothing - Ignoring Unknown Words): 60.17091971583495\n",
            "Perplexity (Add-k Smoothing, k=0.5 - Ignoring Unknown Words): 43.7558536740361\n",
            "Perplexity (Add-k Smoothing, k=3 - Ignoring Unknown Words): 97.7601456201918\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Smoothing and Perplexity Calculations Handling Unknown Words"
      ],
      "metadata": {
        "id": "0XdX_AGzA8hW"
      },
      "id": "0XdX_AGzA8hW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the Unknown Word tag with frequency zero\n",
        "print(\"\\n--- Adding Unknown Word Tag with Zero Frequency ---\")\n",
        "lap_uni_count = defaultdict(int)\n",
        "for word in train_tokens:\n",
        "    lap_uni_count[word] += 1\n",
        "lap_uni_count['unk'] = 0\n",
        "\n",
        "# Replace rare words in training data with 'unk'\n",
        "new_train_tokens = ['unk' if lap_uni_count[word] == 1 else word for word in train_tokens]\n",
        "new_train_bigrams = [(new_train_tokens[i], new_train_tokens[i + 1]) for i in range(len(new_train_tokens) - 1)]\n",
        "\n",
        "# Recomputing bigram counts with 'unk' handling\n",
        "lap_bi_count = defaultdict(int)\n",
        "for bigram in new_train_bigrams:\n",
        "    lap_bi_count[bigram] += 1\n",
        "\n",
        "lap_bi_train_prob, lap_bi_train_log_prob = defaultdict(int), defaultdict(int)\n",
        "for bigram in lap_bi_count:\n",
        "    lap_bi_train_prob[bigram] = (lap_bi_count[bigram] + 1) / (lap_uni_count[bigram[0]] + len(lap_uni_count))\n",
        "    lap_bi_train_log_prob[bigram] = math.log(lap_bi_train_prob[bigram])\n",
        "\n",
        "# Perplexity calculation with Laplace Smoothing for 'unk' handling\n",
        "perplexity_laplace_unk = calculate_perplexity_bigram(lap_bi_train_prob, val_bigrams, lap_bi_train_log_prob)\n",
        "results['Perplexity (Laplace Smoothing - Adding unk with Zero Frequency)'] = perplexity_laplace_unk\n",
        "print(f\"Perplexity (Laplace Smoothing - Adding 'unk' with Zero Frequency): {perplexity_laplace_unk}\")\n",
        "\n",
        "# Add-k Smoothing with 'unk' handling\n",
        "k = 0.5\n",
        "k_bi_train_prob = defaultdict(int)\n",
        "for bigram in lap_bi_count:\n",
        "    k_bi_train_prob[bigram] = (lap_bi_count[bigram] + k) / (lap_uni_count[bigram[0]] + (k * len(lap_uni_count)))\n",
        "\n",
        "k_bi_train_log_prob = defaultdict(int)\n",
        "for bigram in k_bi_train_prob:\n",
        "    k_bi_train_log_prob[bigram] = math.log(k_bi_train_prob[bigram])\n",
        "\n",
        "# Perplexity for Add-k Smoothing with 'unk' handling, k = 0.5\n",
        "perplexity_add_k_0_5_unk = calculate_perplexity_bigram(k_bi_train_prob, val_bigrams, k_bi_train_log_prob)\n",
        "results['Perplexity (Add-k Smoothing, k=0.5 - Adding unk with Zero Frequency)'] = perplexity_add_k_0_5_unk\n",
        "print(f\"Perplexity (Add-k Smoothing, k=0.5 - Adding 'unk' with Zero Frequency): {perplexity_add_k_0_5_unk}\")\n",
        "\n",
        "k = 3\n",
        "k_bi_train_prob = defaultdict(int)\n",
        "for bigram in lap_bi_count:\n",
        "    k_bi_train_prob[bigram] = (lap_bi_count[bigram] + k) / (lap_uni_count[bigram[0]] + (k * len(lap_uni_count)))\n",
        "\n",
        "k_bi_train_log_prob = defaultdict(int)\n",
        "for bigram in k_bi_train_prob:\n",
        "    k_bi_train_log_prob[bigram] = math.log(k_bi_train_prob[bigram])\n",
        "\n",
        "# Perplexity for Add-k Smoothing with 'unk' handling, k = 3\n",
        "perplexity_add_k_3_unk = calculate_perplexity_bigram(k_bi_train_prob, val_bigrams, k_bi_train_log_prob)\n",
        "results['Perplexity (Add-k Smoothing, k=3 - Adding unk with Zero Frequency)'] = perplexity_add_k_3_unk\n",
        "print(f\"Perplexity (Add-k Smoothing, k=3 - Adding 'unk' with Zero Frequency): {perplexity_add_k_3_unk}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMlnzaMlAz8Z",
        "outputId": "e80cfde3-b8f8-4cc0-c381-93935c0b4f03"
      },
      "id": "yMlnzaMlAz8Z",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Adding Unknown Word Tag with Zero Frequency ---\n",
            "Perplexity (Laplace Smoothing - Adding 'unk' with Zero Frequency): 193.1745009886092\n",
            "Perplexity (Add-k Smoothing, k=0.5 - Adding 'unk' with Zero Frequency): 110.32072881036434\n",
            "Perplexity (Add-k Smoothing, k=3 - Adding 'unk' with Zero Frequency): 459.7445797802327\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Smoothing and Perplexity Calculations With Rare Words as Unknown"
      ],
      "metadata": {
        "id": "NirfApmkA4Si"
      },
      "id": "NirfApmkA4Si"
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing Rare Words with Unknown Tag\n",
        "print(\"\\n--- Replacing Rare Words with Unknown Tag ---\")\n",
        "# Replace rare words with 'unk'\n",
        "rare_words = [word for word in lap_uni_count if lap_uni_count[word] == 1]\n",
        "new_train_tokens = [('unk' if token in rare_words else token) for token in train_tokens]\n",
        "new_train_bigrams = [(new_train_tokens[i], new_train_tokens[i + 1]) for i in range(len(new_train_tokens) - 1)]\n",
        "\n",
        "# Recompute counts for training data with 'unk' handling\n",
        "new_unigram_count = defaultdict(int)\n",
        "for token in new_train_tokens:\n",
        "    new_unigram_count[token] += 1\n",
        "\n",
        "new_bigram_count = defaultdict(int)\n",
        "for bigram in new_train_bigrams:\n",
        "    new_bigram_count[bigram] += 1\n",
        "\n",
        "# Calculate Laplace Smoothing for new data with 'unk'\n",
        "laplace_new_bigram_prob = defaultdict(int)\n",
        "laplace_new_bigram_log_prob = defaultdict(int)\n",
        "for bigram in new_bigram_count:\n",
        "    laplace_new_bigram_prob[bigram] = (new_bigram_count[bigram] + 1) / (new_unigram_count[bigram[0]] + len(new_unigram_count))\n",
        "    laplace_new_bigram_log_prob[bigram] = math.log(laplace_new_bigram_prob[bigram])\n",
        "\n",
        "# Perplexity for Laplace Smoothing with Rare Words as 'unk'\n",
        "perplexity_laplace_rare = calculate_perplexity_bigram(laplace_new_bigram_prob, val_bigrams, laplace_new_bigram_log_prob)\n",
        "results['Perplexity (Laplace Smoothing with Rare Words as Unknown)'] = perplexity_laplace_rare\n",
        "print(f\"Perplexity (Laplace Smoothing with Rare Words as Unknown): {perplexity_laplace_rare}\")\n",
        "\n",
        "# Add-k Smoothing with 'unk' handling for rare words and k = 0.5\n",
        "k = 0.5\n",
        "k_rare_bigram_prob = defaultdict(int)  # Initialize bigram probability dictionary\n",
        "k_rare_bigram_log_prob = defaultdict(int)  # Initialize log probability dictionary\n",
        "\n",
        "for bigram in new_bigram_count:\n",
        "    k_rare_bigram_prob[bigram] = (new_bigram_count[bigram] + k) / (new_unigram_count[bigram[0]] + (k * len(new_unigram_count)))\n",
        "    k_rare_bigram_log_prob[bigram] = math.log(k_rare_bigram_prob[bigram])\n",
        "\n",
        "# Perplexity for Add-k Smoothing (k = 0.5) with Rare Words as 'unk'\n",
        "perplexity_add_k_0_5_rare = calculate_perplexity_bigram(k_rare_bigram_prob, val_bigrams, k_rare_bigram_log_prob)\n",
        "results['Perplexity (Add-k Smoothing, k=0.5 - Rare Words as Unknown)'] = perplexity_add_k_0_5_rare\n",
        "print(f\"Perplexity (Add-k Smoothing, k=0.5 - Rare Words as Unknown): {perplexity_add_k_0_5_rare}\")\n",
        "\n",
        "# Add-k Smoothing with 'unk' handling for rare words and k = 3\n",
        "k = 3\n",
        "k_rare_bigram_prob = defaultdict(int)  # Initializing bigram probability dictionary\n",
        "k_rare_bigram_log_prob = defaultdict(int)  # Initializing log probability dictionary\n",
        "\n",
        "for bigram in new_bigram_count:\n",
        "    k_rare_bigram_prob[bigram] = (new_bigram_count[bigram] + k) / (new_unigram_count[bigram[0]] + (k * len(new_unigram_count)))\n",
        "    k_rare_bigram_log_prob[bigram] = math.log(k_rare_bigram_prob[bigram])\n",
        "\n",
        "# Perplexity for Add-k Smoothing (k = 3) with Rare Words as 'unk'\n",
        "perplexity_add_k_3_rare = calculate_perplexity_bigram(k_rare_bigram_prob, val_bigrams, k_rare_bigram_log_prob)\n",
        "results['Perplexity (Add-k Smoothing, k=3 - Rare Words as Unknown)'] = perplexity_add_k_3_rare\n",
        "print(f\"Perplexity (Add-k Smoothing, k=3 - Rare Words as Unknown): {perplexity_add_k_3_rare}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLEAecjhwxwd",
        "outputId": "50050b4e-4b7a-4c60-ebca-7c7f6251b377"
      },
      "id": "fLEAecjhwxwd",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Replacing Rare Words with Unknown Tag ---\n",
            "Perplexity (Laplace Smoothing with Rare Words as Unknown): 135.37235609080957\n",
            "Perplexity (Add-k Smoothing, k=0.5 - Rare Words as Unknown): 91.95803128753613\n",
            "Perplexity (Add-k Smoothing, k=3 - Rare Words as Unknown): 270.69792197096535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final Step - Display All Results"
      ],
      "metadata": {
        "id": "bJGist_3xiPA"
      },
      "id": "bJGist_3xiPA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying all the results stored in the results dictionary\n",
        "print(\"Final Step: Displaying results for all cases (Laplace, Add-k=0.5, Add-k=3, No Smoothing)\")\n",
        "print(\"-------------------------\")\n",
        "for key, value in results.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\nAll results have been displayed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXCk-gGeuCIY",
        "outputId": "a251f5a1-1e19-4028-b1e0-c9bc73dca0e3"
      },
      "id": "dXCk-gGeuCIY",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Step: Displaying results for all cases (Laplace, Add-k=0.5, Add-k=3, No Smoothing)\n",
            "-------------------------\n",
            "Perplexity (Training Dataset Without Any Smoothing): 30.21189738227987\n",
            "Perplexity (Laplace Smoothing): 60.17091971583495\n",
            "Perplexity (Add-k Smoothing k=0.5): 43.7558536740361\n",
            "Perplexity (Add-k Smoothing k=3): 97.7601456201918\n",
            "Perplexity (Laplace Smoothing - Adding unk with Zero Frequency): 193.1745009886092\n",
            "Perplexity (Add-k Smoothing, k=0.5 - Adding unk with Zero Frequency): 110.32072881036434\n",
            "Perplexity (Add-k Smoothing, k=3 - Adding unk with Zero Frequency): 459.7445797802327\n",
            "Perplexity (Laplace Smoothing with Rare Words as Unknown): 135.37235609080957\n",
            "Perplexity (Add-k Smoothing, k=0.5 - Rare Words as Unknown): 91.95803128753613\n",
            "Perplexity (Add-k Smoothing, k=3 - Rare Words as Unknown): 270.69792197096535\n",
            "\n",
            "All results have been displayed.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}